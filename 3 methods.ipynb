{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31274562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the original ADR related People data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('randomly_generated_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a7c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>item_list</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['K31.8']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['1732', '1831', '3123002', '3125501', '4019',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['E87', 'I08', 'I10', 'I27', 'I48', 'I50', 'I6...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['13706-02', '47009-00', '49318-00', '92514-39...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['I48', 'I50', 'I70', 'M80', 'R42', 'Z92', 'Z9...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>91</td>\n",
       "      <td>['96200-00', '96200-00', '96200-00', 'Z51.1', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>1501</td>\n",
       "      <td>['Z45.81']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4856</th>\n",
       "      <td>903</td>\n",
       "      <td>['M45.02', 'M45.04', 'M45.08', 'M45.02']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>1276</td>\n",
       "      <td>['13100-08', '13100-08', '13100-08', '13100-08...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>326</td>\n",
       "      <td>['95550-02', '95550-03', 'F05.9', 'K59.0', 'M1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4859 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      person_id                                          item_list  label\n",
       "0             0                                          ['K31.8']      1\n",
       "1             1  ['1732', '1831', '3123002', '3125501', '4019',...      1\n",
       "2             2  ['E87', 'I08', 'I10', 'I27', 'I48', 'I50', 'I6...      1\n",
       "3             3  ['13706-02', '47009-00', '49318-00', '92514-39...      1\n",
       "4             4  ['I48', 'I50', 'I70', 'M80', 'R42', 'Z92', 'Z9...      1\n",
       "...         ...                                                ...    ...\n",
       "4854         91  ['96200-00', '96200-00', '96200-00', 'Z51.1', ...      0\n",
       "4855       1501                                         ['Z45.81']      0\n",
       "4856        903           ['M45.02', 'M45.04', 'M45.08', 'M45.02']      0\n",
       "4857       1276  ['13100-08', '13100-08', '13100-08', '13100-08...      0\n",
       "4858        326  ['95550-02', '95550-03', 'F05.9', 'K59.0', 'M1...      0\n",
       "\n",
       "[4859 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2e40a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1736418, 9989880)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize the model\n",
    "model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Build the vocabulary\n",
    "model.build_vocab(df['item_list'])\n",
    "\n",
    "# Train the model\n",
    "model.train(df['item_list'], total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715d9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert item lists to vectors\n",
    "def convert_to_vector(item_list):\n",
    "    if len(item_list) > 0:\n",
    "        return np.mean([model.wv[word] for word in item_list], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # return an empty vector if item_list is empty\n",
    "\n",
    "df['vector'] = df['item_list'].apply(convert_to_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43debad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>item_list</th>\n",
       "      <th>label</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['K31.8']</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.1042064, 0.22137798, -0.37011963, 0.200235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['1732', '1831', '3123002', '3125501', '4019',...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.03814807, 0.40642664, -0.06637045, 0.106025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['E87', 'I08', 'I10', 'I27', 'I48', 'I50', 'I6...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.14029576, 0.47451043, -0.16005331, -0.0298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['13706-02', '47009-00', '49318-00', '92514-39...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.013342241, 0.29942247, 0.056784254, 0.00096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['I48', 'I50', 'I70', 'M80', 'R42', 'Z92', 'Z9...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.092795834, 0.41382232, -0.13631779, 0.1648...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>91</td>\n",
       "      <td>['96200-00', '96200-00', '96200-00', 'Z51.1', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.06274616, 0.3131512, 0.13927072, -0.0438267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>1501</td>\n",
       "      <td>['Z45.81']</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0002693534, 0.29829222, -0.42055616, 0.124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4856</th>\n",
       "      <td>903</td>\n",
       "      <td>['M45.02', 'M45.04', 'M45.08', 'M45.02']</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.11171655, 0.21888793, -0.25649709, 0.138578...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>1276</td>\n",
       "      <td>['13100-08', '13100-08', '13100-08', '13100-08...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.044825476, 0.34191805, 0.14123091, 0.078840...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>326</td>\n",
       "      <td>['95550-02', '95550-03', 'F05.9', 'K59.0', 'M1...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.045877576, 0.31249493, -0.014891473, 0.1188...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4859 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      person_id                                          item_list  label  \\\n",
       "0             0                                          ['K31.8']      1   \n",
       "1             1  ['1732', '1831', '3123002', '3125501', '4019',...      1   \n",
       "2             2  ['E87', 'I08', 'I10', 'I27', 'I48', 'I50', 'I6...      1   \n",
       "3             3  ['13706-02', '47009-00', '49318-00', '92514-39...      1   \n",
       "4             4  ['I48', 'I50', 'I70', 'M80', 'R42', 'Z92', 'Z9...      1   \n",
       "...         ...                                                ...    ...   \n",
       "4854         91  ['96200-00', '96200-00', '96200-00', 'Z51.1', ...      0   \n",
       "4855       1501                                         ['Z45.81']      0   \n",
       "4856        903           ['M45.02', 'M45.04', 'M45.08', 'M45.02']      0   \n",
       "4857       1276  ['13100-08', '13100-08', '13100-08', '13100-08...      0   \n",
       "4858        326  ['95550-02', '95550-03', 'F05.9', 'K59.0', 'M1...      0   \n",
       "\n",
       "                                                 vector  \n",
       "0     [-0.1042064, 0.22137798, -0.37011963, 0.200235...  \n",
       "1     [0.03814807, 0.40642664, -0.06637045, 0.106025...  \n",
       "2     [-0.14029576, 0.47451043, -0.16005331, -0.0298...  \n",
       "3     [0.013342241, 0.29942247, 0.056784254, 0.00096...  \n",
       "4     [-0.092795834, 0.41382232, -0.13631779, 0.1648...  \n",
       "...                                                 ...  \n",
       "4854  [0.06274616, 0.3131512, 0.13927072, -0.0438267...  \n",
       "4855  [-0.0002693534, 0.29829222, -0.42055616, 0.124...  \n",
       "4856  [0.11171655, 0.21888793, -0.25649709, 0.138578...  \n",
       "4857  [0.044825476, 0.34191805, 0.14123091, 0.078840...  \n",
       "4858  [0.045877576, 0.31249493, -0.014891473, 0.1188...  \n",
       "\n",
       "[4859 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c729c3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8045267489711934\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060828f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (65.6.3)\n",
      "Requirement already satisfied: numpy in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (1.22.4)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-macosx_10_16_x86_64.whl size=304536 sha256=5e60478bfb67fc8f6e686ce77bf3e92d4e08765e60af850031e3355938040c45\n",
      "  Stored in directory: /Users/tom/Library/Caches/pip/wheels/3f/84/86/c63cf501c46fb575152daee4b937075a5e9b31765c0e620fd4\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff9fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  59\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 1469657 lr:  0.000000 avg.loss:  2.614872 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8189300411522634\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Concatenate all words in 'item_list' to form a sentence\n",
    "df['sentence'] = df['item_list'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save these sentences to a text file\n",
    "with open('sentences.txt', 'w') as f:\n",
    "    for sentence in df['sentence']:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "# Train a FastText model\n",
    "model = fasttext.train_unsupervised('sentences.txt', dim=100)\n",
    "\n",
    "# Use the FastText model to convert item lists to vectors\n",
    "def convert_to_vector(item_list):\n",
    "    return np.mean([model.get_word_vector(word) for word in item_list], axis=0)\n",
    "\n",
    "df['vector'] = df['item_list'].apply(convert_to_vector)\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f711aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (22.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-macosx_10_11_x86_64.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.8/400.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tom/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343fec02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b26f54c72c48fa992772472a09cf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb76d52afd43423fbe39308b820b7b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df54facf4784d929fb657fdde46ea18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a931f45ee11b4b959e68550ac6021eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8415637860082305\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'df' is loaded as before\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get BERT embeddings\n",
    "def get_bert_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get the output from BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # The first element of the output tuple is the last hidden state, i.e., the embeddings\n",
    "    # We will take the mean of the embeddings across tokens as the representation for the sentence\n",
    "    embeddings = outputs[0].mean(dim=1).numpy()\n",
    "    \n",
    "    return embeddings[0]\n",
    "\n",
    "# Concatenate all words in 'item_list' to form a sentence\n",
    "df['sentence'] = df['item_list'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Use the BERT model to convert item lists to vectors\n",
    "df['vector'] = df['sentence'].apply(get_bert_embedding)\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9353a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming the BERT embeddings are stored in the 'vector' column of the 'df' dataframe\n",
    "bert_embeddings = df['vector'].tolist()\n",
    "\n",
    "# Save the embeddings to a file\n",
    "with open('bert_embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(bert_embeddings, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46a12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_embeddings.pkl', 'rb') as f:\n",
    "    X_bert = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "480f7c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Use the TfidfVectorizer to convert item lists to vectors\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2069\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \n\u001b[1;32m   2052\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m-> 2069\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1321\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1318\u001b[0m min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_df\n\u001b[1;32m   1319\u001b[0m max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features\n\u001b[0;32m-> 1321\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1324\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1239\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'df' is loaded as before\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Concatenate all words in 'item_list' to form a sentence\n",
    "df['sentence'] = df['item_list'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Use the TfidfVectorizer to convert item lists to vectors\n",
    "X = vectorizer.fit_transform(df['sentence']).toarray()\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c06673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.7901234567901234\n",
      "Logistic Regression Accuracy: 0.8261316872427984\n",
      "SVM Accuracy: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the embeddings list to a NumPy array\n",
    "X_bert = np.array(df['vector'].tolist())\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bert, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('SVM Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83b3224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.7366255144032922\n",
      "Logistic Regression Accuracy: 0.7417695473251029\n",
      "SVM Accuracy: 0.772633744855967\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize the model\n",
    "model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Build the vocabulary\n",
    "model.build_vocab(df['item_list'])\n",
    "\n",
    "# Train the model\n",
    "model.train(df['item_list'], total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert item lists to vectors\n",
    "def convert_to_vector(item_list):\n",
    "    if len(item_list) > 0:\n",
    "        return np.mean([model.wv[word] for word in item_list], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # return an empty vector if item_list is empty\n",
    "\n",
    "df['vector'] = df['item_list'].apply(convert_to_vector)\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('SVM Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc617c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  59\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 1386119 lr:  0.000000 avg.loss:  2.615921 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.7592592592592593\n",
      "Logistic Regression Accuracy: 0.7006172839506173\n",
      "SVM Accuracy: 0.7438271604938271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Concatenate all words in 'item_list' to form a sentence\n",
    "df['sentence'] = df['item_list'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save these sentences to a text file\n",
    "with open('sentences.txt', 'w') as f:\n",
    "    for sentence in df['sentence']:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "# Train a FastText model\n",
    "model = fasttext.train_unsupervised('sentences.txt', dim=100)\n",
    "\n",
    "# Use the FastText model to convert item lists to vectors\n",
    "def convert_to_vector(item_list):\n",
    "    return np.mean([model.get_word_vector(word) for word in item_list], axis=0)\n",
    "\n",
    "df['vector'] = df['item_list'].apply(convert_to_vector)\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('SVM Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909f645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
